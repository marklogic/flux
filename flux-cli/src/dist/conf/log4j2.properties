# The root logger with appender name
rootLogger = INFO, STDOUT

# Assign STDOUT a valid appender & define its layout
appender.console.name = STDOUT
appender.console.type = Console
appender.console.layout.type = PatternLayout
#appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} [%tn] %p %c: %m%n
appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %c: %m%n

logger.flux.name=com.marklogic.flux
logger.flux.level=INFO

logger.marklogicclient.name=com.marklogic.client
logger.marklogicclient.level=WARN
logger.marklogicspark.name=com.marklogic.spark
logger.marklogicspark.level=INFO

# Set to INFO or DEBUG for langchain4j-specific logging from the MarkLogic Spark connector.
logger.marklogiclangchain4j.name=com.marklogic.langchain4j
logger.marklogiclangchain4j.level=WARN

# Set to DEBUG for langchain4j debug-level logging.
logger.langchain4j.name=dev.langchain4j
logger.langchain4j.level=INFO

# Classifier logging; set to DEBUG or TRACE for logging of requests.
logger.classifier.name=com.marklogic.semaphore.classifier
logger.classifier.level=INFO
logger.smartlogic.name=com.smartlogic.cloud
logger.smartlogic.level=WARN

# Extractor logging
logger.pdfbox.name=org.apache.pdfbox
logger.pdfbox.level=ERROR

# This logs write failures at the "warn" level in a verbose way.
logger.writeBatcherImpl.name=com.marklogic.client.datamovement.impl.WriteBatcherImpl
logger.writeBatcherImpl.level=ERROR

# The following both log errors at the ERROR level, but that is usually too verbose. The user can instead include
# "--stacktrace" to see a single stacktrace when the tool catches an Exception.
logger.sparkutils.name=org.apache.spark.util.Utils
logger.sparkutils.level=FATAL
logger.sparkexecutor.name=org.apache.spark.executor.Executor
logger.sparkexecutor.level=FATAL
logger.sparktasksetmanager.name=org.apache.spark.scheduler.TaskSetManager
logger.sparktasksetmanager.level=FATAL

logger.sparksql.name=org.apache.spark.sql
logger.sparksql.level=FATAL

logger.spark.name=org.apache.spark
logger.spark.level=ERROR
logger.sparkproject.name=org.sparkproject
logger.sparkproject.level=WARN
logger.hadoop.name=org.apache.hadoop
logger.hadoop.level=ERROR
logger.parquet.name=org.apache.parquet
logger.parquet.level=WARN
logger.orc.name=org.apache.orc
logger.orc.level=WARN

# Jena has a fix to ignore "Unrecognized property - 'http://javax.xml.XMLConstants/property/accessExternalDTD'" errors
# from Woodstox, but Hadoop has its own version of Woodstox that Jena isn't aware of. The error is not relevant for
# a user, so it's suppressed here.
logger.jenaxmlinput.name=org.apache.jena.util.JenaXMLInput
logger.jenaxmlinput.level=FATAL

# Quieting this down to avoid a possible error message on Windows.
# See https://stackoverflow.com/questions/41825871/exception-while-deleting-spark-temp-dir-in-windows-7-64-bit/76753660#76753660
# and also https://issues.apache.org/jira/browse/SPARK-12216 for background. It appears to be an unresolved issue with
# Spark on Windows, with the downside being that temp data is not always deleted. It does not have any impact on the
# result of the Flux command.
logger.sparkenv.name=org.apache.spark.storage.DiskBlockManager
logger.sparkenv.level=FATAL

# When using flux-embedding-model-azure-open-ai, the NettyUtility class will log a warning containing:
# "The following Netty versions were found on the classpath". This is due to Azure OpenAI and Spark using different
# versions of Netty. This is mitigated via the shadowing of io.netty in flux-embedding-model-azure-open-ai, but the
# message still appears. This hides it, as there's no action a user can take to address it.
logger.azurenetty.name=com.azure.core.http.netty.implementation.NettyUtility
logger.azurenetty.level=ERROR
