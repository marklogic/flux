plugins {
  id 'net.saliman.properties' version '1.5.2'
  id "application"
  id "jacoco"
  id 'com.gradleup.shadow' version '8.3.3'
  id 'maven-publish'
}

configurations {
  // Defines only those dependencies that we want to include in the assembly/shadow jar that will be used by spark-submit.
  shadowDependencies

  // Defines dependencies to be included in the Flux distribution (in addition to all normal runtime dependencies),
  // but should not be transitive dependencies of the Flux API. Typically, a Flux API user can easily choose to include
  // those transitive dependencies if desired.
  distributionDependencies

  // Since the distribution dependencies are included in Flux, we want to test them. So the test runtime includes all
  // distribution dependencies.
  testImplementation.extendsFrom(distributionDependencies)
}

dependencies {
  implementation "org.apache.spark:spark-sql_2.12:${sparkVersion}"
  implementation "com.marklogic:marklogic-spark-connector:${connectorVersion}"
  implementation "info.picocli:picocli:4.7.6"

  // The shadow jar intended for usage with spark-submit needs the above "core" libraries as well, but not the Spark
  // API since spark-submit provides that.
  // Any other implementation dependencies below this are either provided by spark-submit or can be easily included
  // with spark-submit via its support for downloading dependencies expressed by Maven coordinates.
  shadowDependencies "com.marklogic:marklogic-spark-connector:${connectorVersion}"
  shadowDependencies "info.picocli:picocli:4.7.6"

  // Spark 3.4.3 depends on Hadoop 3.3.4, which depends on AWS SDK 1.12.262. As of August 2024, all public releases of
  // Spark - through 3.5.1 - depend on Hadoop 3.3.4 as well.
  // Hadoop 3.3.4 is flagged with a high security vulnerability - https://nvd.nist.gov/vuln/detail/CVE-2023-26031 .
  // That CVE notes that the vulnerability is awaiting reanalysis.
  //
  // However, the CVE includes the text "If the YARN cluster is accepting work from remote (authenticated) users".
  // Flux does not create a YARN cluster. It uses a local standalone Spark cluster. A user may choose to use Flux with
  // spark-submit against a Spark cluster using YARN, but at that point, the user is 100% responsible for how they
  // configure their own Spark cluster.
  implementation("org.apache.hadoop:hadoop-aws:3.3.4") {
    // We don't include the entire aws-java-sdk-bundle, as that clocks in as a single 380mb jar. We only need the S3
    // portion of the AWS SDK.
    exclude module: "aws-java-sdk-bundle"
  }

  // Depending on 262, which is what hadoop-aws 3.3.4 depends on.
  implementation "com.amazonaws:aws-java-sdk-s3:1.12.262"

  // With version 262 of the AWS SDK, we get a NoClassDefFoundError if this dynamodb module is not included.
  implementation "com.amazonaws:aws-java-sdk-dynamodb:1.12.262"

  // Adds support for Azure Storage. We may want these to be expressed using variants instead so that at least a
  // Gradle user of the API can ask for e.g. S3 or Azure Storage support without those dependencies being included
  // by default.
  implementation "org.apache.hadoop:hadoop-azure:3.3.4"
  // For Blob Storage
  implementation "com.microsoft.azure:azure-storage:8.6.6"
  // For Data Lake Storage Gen2
  implementation "com.azure:azure-storage-file-datalake:12.23.1"

  implementation "org.apache.hadoop:hadoop-client:3.3.4"

  // Spark doesn't include Avro support by default, so need to bring this in.
  implementation "org.apache.spark:spark-avro_2.12:${sparkVersion}"

  distributionDependencies "org.apache.tika:tika-parser-microsoft-module:${tikaVersion}"
  distributionDependencies "org.apache.tika:tika-parser-pdf-module:${tikaVersion}"

  testImplementation project(":flux-tests-api")

  // For testing custom commands with a 3rd party connector.
  testImplementation "com.databricks:spark-xml_2.12:0.18.0"
}

javadoc {
  include "com/marklogic/flux/api/**"
}

tasks.register("deleteJavadoc", Delete) {
  delete "../docs/assets/javadoc"
}
tasks.register("copyJavadoc", Copy) {
  from layout.buildDirectory.dir("docs/javadoc")
  into "../docs/assets/javadoc"
  rename { filename -> filename.endsWith(".md") ? filename.replace(".md", ".txt") : filename }
}
copyJavadoc.mustRunAfter deleteJavadoc, javadoc

// Must run this task with Java 17.
tasks.register("updateJavadoc")
updateJavadoc.dependsOn deleteJavadoc, javadoc, copyJavadoc

// Configures the Gradle distribution plugin.
// See https://docs.gradle.org/current/userguide/distribution_plugin.html for more information.
// The distribution forms the contents of the application zip.
distributions {
  main {
    distributionBaseName = "marklogic-flux"
    contents {
      from("..") {
        include "LICENSE"
        include "NOTICE.txt"
      }
      from("hadoop") {
        include "hadoop.dll"
        include "winutils.exe"
        include "msvcr100.dll"
        include "msvcr120.dll"
        into "bin"
      }
      from("hadoop") {
        include "libhadoop.so"
        into "lib/native"
      }
      // The distributionDependencies configuration allows us to include dependencies that should be part of the Flux
      // distribution, but not part of the default Flux API set of dependencies.
      into("lib") {
        from configurations.distributionDependencies
        // It's expected to run into duplicates with dependencies already on the runtime classpath, so any duplicates
        // can safely be excluded.
        duplicatesStrategy = DuplicatesStrategy.EXCLUDE
      }
    }
  }
}

tasks.register("deleteEmbeddingModelJars", Delete) {
  delete fileTree("src/dist/ext").matching {
    include "*.jar"
  }
}
// We don't want to include the embedding model jars in the distribution zip, which greatly increases the size of the
// zip. Users wishing to generate embeddings are required to download the appropriate jar and add it to the "ext"
// folder in their Flux installation.
distZip.dependsOn deleteEmbeddingModelJars

tasks.register("copyEmbeddingModelJarsIntoDistribution", Copy) {
  description = "Intended for internal usage when building the Flux zip with all the embedding model integrations included."
  dependsOn ":flux-embedding-model-azure-open-ai:shadowJar"
  dependsOn ":flux-embedding-model-bedrock:shadowJar"
  dependsOn ":flux-embedding-model-minilm:shadowJar"
  dependsOn ":flux-embedding-model-ollama:shadowJar"
  from("../flux-embedding-model-azure-open-ai/build/libs")
  from("../flux-embedding-model-bedrock/build/libs")
  from("../flux-embedding-model-minilm/build/libs")
  from("../flux-embedding-model-ollama/build/libs")
  into "src/dist/ext"
}

// Gradle complains without these ensuring the order of the tasks.
distZip.mustRunAfter copyEmbeddingModelJarsIntoDistribution
distTar.mustRunAfter copyEmbeddingModelJarsIntoDistribution
installDist.mustRunAfter copyEmbeddingModelJarsIntoDistribution

// Configures the Gradle application plugin.
// See https://docs.gradle.org/current/userguide/application_plugin.html for more information.
application {
  mainClass = "com.marklogic.flux.cli.Main"
  applicationDefaultJvmArgs = [
    // Removes warnings due to Spark performing "illegal reflective access".
    '--add-opens', 'java.base/java.nio=ALL-UNNAMED',
    '--add-opens', 'java.base/java.net=ALL-UNNAMED',
    '--add-opens', 'java.base/java.lang=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util.concurrent=ALL-UNNAMED',

    // Required for Java 17 support.
    '--add-opens', 'java.base/sun.nio.ch=ALL-UNNAMED',

    // Required for some Spark SQL operations.
    '--add-opens', 'java.base/sun.util.calendar=ALL-UNNAMED',

    // For Spark's SerializationDebugger when using Java 17.
    '--add-opens', 'java.base/sun.security.action=ALL-UNNAMED',

    // Allows a reflective access by org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassReflection .
    // This warning otherwise shows on Java 11 but not Java 17.
    "--add-opens", "java.base/java.io=ALL-UNNAMED",

    // Disable Netty's native SSL support to avoid tcnative loading errors when using Azure OpenAI embedding model.
    // Netty will fall back to pure Java SSL implementation, which works fine for our use case.
    // Advanced users can remove this property from the startup script to enable native SSL for potential performance gains.
    "-Dio.netty.handler.ssl.noOpenSsl=true"
  ]
}

// Modifies the application's start script to use our modified one that adds jars in the "./ext" folder to the classpath.
startScripts {
  unixStartScriptGenerator.template = resources.text.fromFile('scripts/start-script.txt')
  windowsStartScriptGenerator.template = resources.text.fromFile('scripts/start-script-windows.txt')
  applicationName = "flux"
}

tasks.register("createVersionFile") {
  description = "Create a gitignored file that is available on the classpath for use by the CLI's 'version' command."
  doLast {
    file("src/main/resources/flux-version.properties").text = "version=${version}\nbuildTime=${new Date().format("yyyy-MM-dd HH:mm:ss")}"
  }
}

// Only need the version file in the context of the CLI, not the API.
installDist.dependsOn createVersionFile
distZip.dependsOn createVersionFile
distTar.dependsOn createVersionFile
test.dependsOn createVersionFile

tasks.register("deleteTool", Delete) {
  delete "../flux"
}
tasks.register("buildTool", Copy) {
  from layout.buildDirectory.dir("install/flux")
  into "../flux"
}
buildTool.dependsOn installDist, deleteTool

tasks.register("buildToolForGettingStarted", Copy) {
  description = "For testing Flux with the getting-started example project."
  from layout.buildDirectory.dir("install")
  into "../examples/getting-started"
}
buildToolForGettingStarted.dependsOn installDist

test {
  finalizedBy jacocoTestReport
}

jacocoTestReport {
  dependsOn test
  reports {
    xml.required = true
  }
}

// See https://imperceptiblethoughts.com/shadow/configuration/dependencies/ .
shadowJar {
  configurations = [project.configurations.shadowDependencies]
  archiveBaseName = "marklogic-flux"
}

// Publishing setup - see https://docs.gradle.org/current/userguide/publishing_setup.html .
java {
  withJavadocJar()
  withSourcesJar()
}

publishing {
  publications {
    mainJava(MavenPublication) {
      groupId = group
      // Using a more fitting name of "flux-api". May eventually break out the current "flux-cli" module into
      // multiple Gradle subprojects, such as "flux-api" and "flux-cli".
      artifactId = "flux-api"
      version = version
      from components.java
      pom {
        name = "${group}:flux-api"
        description = "Flux API for data movement with MarkLogic"
        packaging = "jar"
        url = "https://github.com/marklogic/flux"
        licenses {
          license {
            name = "The Apache License, Version 2.0"
            url = "http://www.apache.org/licenses/LICENSE-2.0.txt"
          }
        }
        developers {
          developer {
            id = "marklogic"
            name = "MarkLogic Github Contributors"
            email = "general@developer.marklogic.com"
            organization = "MarkLogic"
            organizationUrl = "https://www.marklogic.com"
          }
        }
        scm {
          url = "git@github.com:marklogic/flux.git"
          connection = "scm:git@github.com:marklogic/flux.git"
          developerConnection = "scm:git@github.com:marklogic/flux.git"
        }
      }
    }
  }
  repositories {
    maven {
      if (project.hasProperty("mavenUser")) {
        credentials {
          username mavenUser
          password mavenPassword
        }
        url publishUrl
        allowInsecureProtocol = true
      } else {
        name = "central"
        url = mavenCentralUrl
        credentials {
          username mavenCentralUsername
          password mavenCentralPassword
        }
      }
    }
  }
}

// Multiple shadow plugin users have requested the ability to not have the shadow jar published by default. This
// does the trick for that. See https://docs.gradle.org/current/userguide/publishing_customization.html .
// Also see https://github.com/johnrengelman/shadow/issues/586#issuecomment-70837559 for the shadow jar issue.
components.java.withVariantsFromConfiguration(configurations.shadowRuntimeElements) {
  skip()
}
