plugins {
  id 'net.saliman.properties' version '1.5.2'
  id "application"
  id "jacoco"
  id "org.sonarqube" version "6.0.1.5171"
  id 'com.gradleup.shadow' version '8.3.3'
  id 'maven-publish'
}

configurations {
  // Defines only those dependencies that we want to include in the assembly/shadow jar that will be used by spark-submit.
  shadowDependencies
}

configurations.all {
  // By default, Spark 3.5.3 does not include the log4j 1.x dependency via its zookeeper dependency. But somehow, by
  // adding hadoop-client 3.3.4 to the mix, the log4j 1.x dependency comes via the zookeeper 3.6.3 dependency. Per
  // the release notes at https://zookeeper.apache.org/doc/r3.6.4/releasenotes.html, using zookeeper 3.6.4 - which
  // removes log4j 1.x, thus avoiding the major CVE associated with log4j 1.x - appears safe, which is confirmed by
  // tests as well.
  resolutionStrategy {
    force "org.apache.zookeeper:zookeeper:3.6.4"
  }
}

dependencies {
  implementation("org.apache.spark:spark-sql_2.12:3.5.3") {
    // The rocksdbjni dependency weighs in at 50mb and so far does not appear necessary for our use of Spark.
    exclude module: "rocksdbjni"
  }

  implementation "com.marklogic:marklogic-spark-connector:2.5.0"
  implementation "info.picocli:picocli:4.7.6"

  // Spark 3.4.3 depends on Hadoop 3.3.4, which depends on AWS SDK 1.12.262. As of August 2024, all public releases of
  // Spark - through 3.5.1 - depend on Hadoop 3.3.4 as well.
  // Hadoop 3.3.4 is flagged with a high security vulnerability - https://nvd.nist.gov/vuln/detail/CVE-2023-26031 .
  // That CVE notes that the vulnerability is awaiting reanalysis.
  //
  // However, the CVE includes the text "If the YARN cluster is accepting work from remote (authenticated) users".
  // Flux does not create a YARN cluster. It uses a local standalone Spark cluster. A user may choose to use Flux with
  // spark-submit against a Spark cluster using YARN, but at that point, the user is 100% responsible for how they
  // configure their own Spark cluster.
  implementation("org.apache.hadoop:hadoop-aws:3.3.4") {
    // We don't include the entire aws-java-sdk-bundle, as that clocks in as a single 380mb jar. We only need the S3
    // portion of the AWS SDK.
    exclude module: "aws-java-sdk-bundle"
  }

  // Depending on 262, which is what hadoop-aws 3.3.4 depends on.
  implementation "com.amazonaws:aws-java-sdk-s3:1.12.262"

  // With version 262 of the AWS SDK, we get a NoClassDefFoundError if this dynamodb module is not included.
  implementation "com.amazonaws:aws-java-sdk-dynamodb:1.12.262"

  implementation "org.apache.hadoop:hadoop-client:3.3.4"

  // Spark doesn't include Avro support by default, so need to bring this in.
  implementation "org.apache.spark:spark-avro_2.12:3.5.3"

  testImplementation("com.marklogic:marklogic-junit5:1.5.0") {
    // Use the Java Client in the connector jar.
    exclude module: "marklogic-client-api"
  }

  // Used for tests involving JDBC. Spring JDBC greatly simplifies executing SQL queries.
  testImplementation "org.springframework:spring-jdbc:5.3.39"
  testImplementation "org.postgresql:postgresql:42.7.4"

  // For testing custom commands with a 3rd party connector.
  testImplementation "com.databricks:spark-xml_2.12:0.18.0"

  // For configuring two-way SSL in tests.
  testImplementation("com.marklogic:ml-app-deployer:5.0.0") {
    // Use the Java Client in the connector jar.
    exclude module: "marklogic-client-api"
  }

  // Using Apache HttpClient for connecting to the MarkLogic Manage API.
  testImplementation 'org.apache.httpcomponents:httpclient:4.5.14'
  // Forcing HttpClient to use this to address https://snyk.io/vuln/SNYK-JAVA-COMMONSCODEC-561518 .
  testImplementation 'commons-codec:commons-codec:1.17.1'

  // The Gradle docs - https://docs.gradle.org/current/userguide/cross_project_publications.html#sec:simple-sharing-artifacts-between-projects -
  // are not keen on this practice of depending on another project's configuration. But this seems fine for a test
  // dependency, and testing has shown that Gradle will construct each project's shadowJar if it does not already exist.
  testImplementation project(path: ":flux-embedding-model-minilm", configuration: "shadow")
  testImplementation project(path: ":flux-embedding-model-azure-open-ai", configuration: "shadow")
  testImplementation project(path: ":flux-embedding-model-ollama", configuration: "shadow")

  shadowDependencies "com.marklogic:marklogic-spark-connector:2.5-SNAPSHOT"
  shadowDependencies "info.picocli:picocli:4.7.6"
}

javadoc {
  include "com/marklogic/flux/api/**"
}

tasks.register("deleteJavadoc", Delete) {
  delete "../docs/assets/javadoc"
}
tasks.register("copyJavadoc", Copy) {
  from layout.buildDirectory.dir("docs/javadoc")
  into "../docs/assets/javadoc"
  rename { filename -> filename.endsWith(".md") ? filename.replace(".md", ".txt") : filename }
}
copyJavadoc.mustRunAfter deleteJavadoc, javadoc

// Must run this task with Java 17.
tasks.register("updateJavadoc")
updateJavadoc.dependsOn deleteJavadoc, javadoc, copyJavadoc

// Configures the Gradle distribution plugin.
// See https://docs.gradle.org/current/userguide/distribution_plugin.html for more information.
// The distribution forms the contents of the application zip.
distributions {
  main {
    distributionBaseName = "marklogic-flux"
    contents {
      from("..") {
        include "LICENSE"
        include "NOTICE.txt"
      }
      from("hadoop") {
        include "hadoop.dll"
        include "winutils.exe"
        include "msvcr100.dll"
        include "msvcr120.dll"
        into "bin"
      }
      from("hadoop") {
        include "libhadoop.so"
        into "lib/native"
      }
    }
  }
}

tasks.register("copyEmbeddingModelJarsIntoDistribution", Copy) {
  description = "Intended for internal usage when building the Flux zip with all the embedding model integrations included."
  dependsOn ":flux-embedding-model-azure-open-ai:shadowJar"
  dependsOn ":flux-embedding-model-minilm:shadowJar"
  dependsOn ":flux-embedding-model-ollama:shadowJar"
  from("../flux-embedding-model-azure-open-ai/build/libs")
  from("../flux-embedding-model-minilm/build/libs")
  from("../flux-embedding-model-ollama/build/libs")
  into "src/dist/ext"
}

// Gradle complains without these ensuring the order of the tasks.
distZip.mustRunAfter copyEmbeddingModelJarsIntoDistribution
distTar.mustRunAfter copyEmbeddingModelJarsIntoDistribution
installDist.mustRunAfter copyEmbeddingModelJarsIntoDistribution

// Configures the Gradle application plugin.
// See https://docs.gradle.org/current/userguide/application_plugin.html for more information.
application {
  mainClass = "com.marklogic.flux.cli.Main"
  applicationDefaultJvmArgs = [
    // Removes warnings due to Spark performing "illegal reflective access".
    '--add-opens', 'java.base/java.nio=ALL-UNNAMED',
    '--add-opens', 'java.base/java.net=ALL-UNNAMED',
    '--add-opens', 'java.base/java.lang=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util.concurrent=ALL-UNNAMED',

    // Required for Java 17 support.
    '--add-opens', 'java.base/sun.nio.ch=ALL-UNNAMED',

    // Required for some Spark SQL operations.
    '--add-opens', 'java.base/sun.util.calendar=ALL-UNNAMED',

    // For Spark's SerializationDebugger when using Java 17.
    '--add-opens', 'java.base/sun.security.action=ALL-UNNAMED',

    // Allows a reflective access by org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassReflection .
    // This warning otherwise shows on Java 11 but not Java 17.
    "--add-opens", "java.base/java.io=ALL-UNNAMED"
  ]
}

// Modifies the application's start script to use our modified one that adds jars in the "./ext" folder to the classpath.
startScripts {
  unixStartScriptGenerator.template = resources.text.fromFile('scripts/start-script.txt')
  windowsStartScriptGenerator.template = resources.text.fromFile('scripts/start-script-windows.txt')
  applicationName = "flux"
}

tasks.register("createVersionFile") {
  description = "Create a gitignored file that is available on the classpath for use by the CLI's 'version' command."
  doLast {
    file("src/main/resources/flux-version.properties").text = "version=${version}\nbuildTime=${new Date().format("yyyy-MM-dd HH:mm:ss")}"
  }
}

// Only need the version file in the context of the CLI, not the API.
installDist.dependsOn createVersionFile
distZip.dependsOn createVersionFile
distTar.dependsOn createVersionFile
test.dependsOn createVersionFile

tasks.register("deleteTool", Delete) {
  delete "../flux"
}
tasks.register("buildTool", Copy) {
  from layout.buildDirectory.dir("install/flux")
  into "../flux"
}
buildTool.dependsOn installDist, deleteTool

tasks.register("buildToolForGettingStarted", Copy) {
  description = "For testing Flux with the getting-started example project."
  from layout.buildDirectory.dir("install")
  into "../examples/getting-started"
}
buildToolForGettingStarted.dependsOn installDist

test {
  finalizedBy jacocoTestReport
  jvmArgs = [
    // Needed for all Java 17 testing.
    "--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED",

    // For Spark's SerializationDebugger when using Java 17. See ReprocessTest for one example of why this is needed.
    "--add-opens", "java.base/sun.security.action=ALL-UNNAMED",

    // Needed by the JDBC tests.
    "--add-opens", "java.base/sun.util.calendar=ALL-UNNAMED",

    // Needed by CustomImportTest
    "--add-opens", "java.base/java.io=ALL-UNNAMED",
    "--add-opens", "java.base/sun.nio.cs=ALL-UNNAMED"
  ]
}

jacocoTestReport {
  dependsOn test
  reports {
    xml.required = true
  }
}

sonar {
  properties {
    property "sonar.projectKey", "flux"
    property "sonar.host.url", "http://localhost:9000"
    // Avoids a warning from Gradle.
    property "sonar.gradle.skipCompile", "true"
  }
}

// See https://imperceptiblethoughts.com/shadow/configuration/dependencies/ .
shadowJar {
  configurations = [project.configurations.shadowDependencies]
  archiveBaseName = "marklogic-flux"
}

// Publishing setup - see https://docs.gradle.org/current/userguide/publishing_setup.html .
java {
  withJavadocJar()
  withSourcesJar()
}

publishing {
  publications {
    mainJava(MavenPublication) {
      groupId = group
      // Using a more fitting name of "flux-api". May eventually break out the current "flux-cli" module into
      // multiple Gradle subprojects, such as "flux-api" and "flux-cli".
      artifactId = "flux-api"
      version = version
      from components.java
      pom {
        name = "${group}:flux-api"
        description = "Flux API for data movement with MarkLogic"
        packaging = "jar"
        url = "https://github.com/marklogic/flux"
        licenses {
          license {
            name = "The Apache License, Version 2.0"
            url = "http://www.apache.org/licenses/LICENSE-2.0.txt"
          }
        }
        developers {
          developer {
            id = "marklogic"
            name = "MarkLogic Github Contributors"
            email = "general@developer.marklogic.com"
            organization = "MarkLogic"
            organizationUrl = "https://www.marklogic.com"
          }
        }
        scm {
          url = "git@github.com:marklogic/flux.git"
          connection = "scm:git@github.com:marklogic/flux.git"
          developerConnection = "scm:git@github.com:marklogic/flux.git"
        }
      }
    }
  }
  repositories {
    maven {
      if (project.hasProperty("mavenUser")) {
        credentials {
          username mavenUser
          password mavenPassword
        }
        url publishUrl
        allowInsecureProtocol = true
      } else {
        name = "central"
        url = mavenCentralUrl
        credentials {
          username mavenCentralUsername
          password mavenCentralPassword
        }
      }
    }
  }
}

// Multiple shadow plugin users have requested the ability to not have the shadow jar published by default. This
// does the trick for that. See https://docs.gradle.org/current/userguide/publishing_customization.html .
// Also see https://github.com/johnrengelman/shadow/issues/586#issuecomment-70837559 for the shadow jar issue.
components.java.withVariantsFromConfiguration(configurations.shadowRuntimeElements) {
  skip()
}
