plugins {
  id 'net.saliman.properties' version '1.5.2'
  id "application"
  id "jacoco"
  id 'com.gradleup.shadow' version '8.3.3'
  id 'maven-publish'
}

configurations {
  // Defines only those dependencies that we want to include in the assembly/shadow jar that will be used by spark-submit.
  shadowDependencies

  all {
    resolutionStrategy {
      // Resolves a medium CVE that comes from json-unit-assertj .
      force "net.minidev:json-smart:2.5.2"
    }
  }
}

dependencies {
  constraints {
    // Ensures the two top-level Hadoop dependencies of Spark will be 3.4.2, which fixes several CVEs found in 3.4.1.
    implementation("org.apache.hadoop:hadoop-client-api:${hadoopVersion}") {
      because "Force Spark's Hadoop dependencies from 3.4.1 to ${hadoopVersion}"
    }
    implementation("org.apache.hadoop:hadoop-client-runtime:${hadoopVersion}") {
      because "Force Spark's Hadoop dependencies from 3.4.1 to ${hadoopVersion}"
    }
    implementation("org.codehaus.janino:janino:3.1.12") {
      because "Bumping from 3.1.9 (what Spark SQL 4.1.0-preview1 depends on) to 3.1.12 to eliminate CVEs."
    }
    implementation("org.eclipse.jetty:jetty-util-ajax:9.4.58.v20250814") {
      because "Bumping to latest 9.x to eliminate CVEs; this is brought in by hadoop-azure."
    }
    implementation("org.apache.zookeeper:zookeeper:3.9.4") {
      because "Addresses CVE-2025-58457; Spark depends on 3.9.3, and the CVE is fixed in 3.9.4."
    }
    // Not yet defining constraints for io.netty, as doing so would require several separate modules to be defined
    // here. The hope is that the 4.1.0 final release will use the latest patch version of io.netty and thus no
    // constraints or top-level resolution strategies will be needed.
  }

  implementation ("org.apache.spark:spark-sql_2.13:${sparkVersion}") {
    // The rocksdbjni dependency weighs in at 50mb and so far does not appear necessary for our use of Spark.
    exclude module: "rocksdbjni"
  }

  implementation "com.marklogic:marklogic-spark-connector:${connectorVersion}"
  implementation "info.picocli:picocli:${picocliVersion}"

  // Need this for compilation as it's not an API dependency of the Spark connector.
  compileOnly "com.marklogic:marklogic-client-api:8.0.0"

  // The shadow jar intended for usage with spark-submit needs the above "core" libraries as well, but not the Spark
  // API since spark-submit provides that.
  // Any other implementation dependencies below this are either provided by spark-submit or can be easily included
  // with spark-submit via its support for downloading dependencies expressed by Maven coordinates.
  shadowDependencies "com.marklogic:marklogic-spark-connector:${connectorVersion}"
  shadowDependencies "info.picocli:picocli:${picocliVersion}"

  implementation("org.apache.hadoop:hadoop-aws:${hadoopVersion}") {
    // Excluded because it's a single 558mb that balloons the Flux distribution in size.
    exclude group: "software.amazon.awssdk"
  }

  // Based on testing so far, this appears to suffice for allowing hadoop-aws to read from and write to S3.
  // Using 2.29.52, which is what Hadoop 3.4.2 depends on.
  implementation "software.amazon.awssdk:s3-transfer-manager:2.29.52"

  // Adds support for Azure Storage. We may want these to be expressed using variants instead so that at least a
  // Gradle user of the API can ask for e.g. S3 or Azure Storage support without those dependencies being included
  // by default.
  implementation "org.apache.hadoop:hadoop-azure:${hadoopVersion}"
  // For Blob Storage
  implementation "com.microsoft.azure:azure-storage:8.6.6"
  // For Data Lake Storage Gen2
  implementation ("com.azure:azure-storage-file-datalake:12.23.1") {
    // Exclude the Jackson dependencies that Spark already provides, and we need the version required by Spark.
    // This exclusion can be tested by publishing flux-api locally and running the test programs in
    // examples/client-project.
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.datatype"
  }

  // Spark doesn't include Avro support by default, so need to bring this in.
  implementation "org.apache.spark:spark-avro_2.13:${sparkVersion}"

  // Optional dependencies - included in distribution but marked as optional in POM
  implementation("org.apache.tika:tika-parser-microsoft-module:${tikaVersion}") {
    // Will be marked as optional in the POM
  }
  implementation("org.apache.tika:tika-parser-pdf-module:${tikaVersion}") {
    // Will be marked as optional in the POM
  }

  // Ensuring 8.0.0 is used on the test classpath.
  // Can remove this once marklogic-junit5 is upgraded to use Java Client 8.0.
  testImplementation ("com.marklogic:marklogic-client-api:8.0.0") {
    // Need to use the versions of Jackson preferred by Spark.
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"
  }

  // For configuring two-way SSL in tests.
  testImplementation ("com.marklogic:ml-app-deployer:6.1.0") {
    // Prefer Spark's versions of Jackson.
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"
  }

  // Automatic loading of test framework implementation dependencies is deprecated.
  // https://docs.gradle.org/current/userguide/upgrading_version_8.html#test_framework_implementation_dependencies
  // Without this, once using JUnit 5.12 or higher, Gradle will not find any tests and report an error of:
  // org.junit.platform.commons.JUnitException: TestEngine with ID 'junit-jupiter' failed to discover tests
  testRuntimeOnly "org.junit.platform:junit-platform-launcher:1.14.0"

  testImplementation "com.marklogic:marklogic-junit5:2.0-SNAPSHOT"

  // Used for tests involving JDBC. Spring JDBC greatly simplifies executing SQL queries.
  testImplementation "org.springframework:spring-jdbc:6.2.11"
  testImplementation "org.postgresql:postgresql:42.7.8"

  // Required for compile-time references in the tests to langchain4j classes.
  testImplementation ("dev.langchain4j:langchain4j:${langchain4jVersion}") {
    // Prefer Spark's versions of Jackson.
    exclude group: "com.fasterxml.jackson.core"
  }

  // The Gradle docs - https://docs.gradle.org/current/userguide/cross_project_publications.html#sec:simple-sharing-artifacts-between-projects -
  // are not keen on this practice of depending on another project's configuration. But this seems fine for a test
  // dependency, and testing has shown that Gradle will construct each project's shadowJar if it does not already exist.
  testImplementation project(path: ":flux-embedding-model-minilm", configuration: "shadow")
  testImplementation project(path: ":flux-embedding-model-azure-open-ai", configuration: "shadow")
  testImplementation project(path: ":flux-embedding-model-ollama", configuration: "shadow")

  // For deep equals on JSON with diff.
  testImplementation "net.javacrumbs.json-unit:json-unit-assertj:4.1.1"

  // For deep equals on XML with diff.
  testImplementation "org.xmlunit:xmlunit-core:2.10.4"
}

javadoc {
  include "com/marklogic/flux/api/**"
}

tasks.register("deleteJavadoc", Delete) {
  delete "../docs/assets/javadoc"
}
tasks.register("copyJavadoc", Copy) {
  from layout.buildDirectory.dir("docs/javadoc")
  into "../docs/assets/javadoc"
  rename { filename -> filename.endsWith(".md") ? filename.replace(".md", ".txt") : filename }
}
copyJavadoc.mustRunAfter deleteJavadoc, javadoc

// Must run this task with Java 17.
tasks.register("updateJavadoc")
updateJavadoc.dependsOn deleteJavadoc, javadoc, copyJavadoc

// Configures the Gradle distribution plugin.
// See https://docs.gradle.org/current/userguide/distribution_plugin.html for more information.
// The distribution forms the contents of the application zip.
distributions {
  main {
    distributionBaseName = "marklogic-flux"
    contents {
      from("..") {
        include "LICENSE"
        include "NOTICE.txt"
      }
      from("hadoop") {
        include "hadoop.dll"
        include "winutils.exe"
        include "msvcr100.dll"
        include "msvcr120.dll"
        into "bin"
      }
      from("hadoop") {
        include "libhadoop.so"
        into "lib/native"
      }
    }
  }
}

tasks.register("deleteEmbeddingModelJars", Delete) {
  delete fileTree("src/dist/ext").matching {
    include "*.jar"
  }
}
// We don't want to include the embedding model jars in the distribution zip, which greatly increases the size of the
// zip. Users wishing to generate embeddings are required to download the appropriate jar and add it to the "ext"
// folder in their Flux installation.
distZip.dependsOn deleteEmbeddingModelJars

tasks.register("copyEmbeddingModelJarsIntoDistribution", Copy) {
  description = "Intended for internal usage when building the Flux zip with all the embedding model integrations included."
  dependsOn ":flux-embedding-model-azure-open-ai:shadowJar"
  dependsOn ":flux-embedding-model-minilm:shadowJar"
  dependsOn ":flux-embedding-model-ollama:shadowJar"
  from("../flux-embedding-model-azure-open-ai/build/libs")
  from("../flux-embedding-model-minilm/build/libs")
  from("../flux-embedding-model-ollama/build/libs")
  into "src/dist/ext"
}

// Gradle complains without these ensuring the order of the tasks.
distZip.mustRunAfter copyEmbeddingModelJarsIntoDistribution
distTar.mustRunAfter copyEmbeddingModelJarsIntoDistribution
installDist.mustRunAfter copyEmbeddingModelJarsIntoDistribution

// Configures the Gradle application plugin.
// See https://docs.gradle.org/current/userguide/application_plugin.html for more information.
application {
  mainClass = "com.marklogic.flux.cli.Main"
  applicationDefaultJvmArgs = [
    // Removes warnings due to Spark performing "illegal reflective access".
    '--add-opens', 'java.base/java.nio=ALL-UNNAMED',
    '--add-opens', 'java.base/java.net=ALL-UNNAMED',
    '--add-opens', 'java.base/java.lang=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util=ALL-UNNAMED',
    '--add-opens', 'java.base/java.util.concurrent=ALL-UNNAMED',

    // Required for Java 17 support.
    '--add-opens', 'java.base/sun.nio.ch=ALL-UNNAMED',

    // Required for some Spark SQL operations.
    '--add-opens', 'java.base/sun.util.calendar=ALL-UNNAMED',

    // For Spark's SerializationDebugger when using Java 17.
    '--add-opens', 'java.base/sun.security.action=ALL-UNNAMED',

    // Allows a reflective access by org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassReflection .
    // This warning otherwise shows on Java 11 but not Java 17.
    "--add-opens", "java.base/java.io=ALL-UNNAMED",

    // Disable Netty's native SSL support to avoid tcnative loading errors when using Azure OpenAI embedding model.
    // Netty will fall back to pure Java SSL implementation, which works fine for our use case.
    // Advanced users can remove this property from the startup script to enable native SSL for potential performance gains.
    "-Dio.netty.handler.ssl.noOpenSsl=true"
  ]
}

// Modifies the application's start script to use our modified one that adds jars in the "./ext" folder to the classpath.
startScripts {
  unixStartScriptGenerator.template = resources.text.fromFile('scripts/start-script.txt')
  windowsStartScriptGenerator.template = resources.text.fromFile('scripts/start-script-windows.txt')
  applicationName = "flux"
}

tasks.register("createVersionFile") {
  description = "Create a gitignored file that is available on the classpath for use by the CLI's 'version' command."
  doLast {
    file("src/main/resources/flux-version.properties").text = "version=${version}\nbuildTime=${new Date().format("yyyy-MM-dd HH:mm:ss")}"
  }
}

// Only need the version file in the context of the CLI, not the API.
installDist.dependsOn createVersionFile
distZip.dependsOn createVersionFile
distTar.dependsOn createVersionFile
test.dependsOn createVersionFile

tasks.register("deleteTool", Delete) {
  delete "../flux"
}
tasks.register("buildTool", Copy) {
  from layout.buildDirectory.dir("install/flux")
  into "../flux"
}
buildTool.dependsOn installDist, deleteTool

tasks.register("buildToolForGettingStarted", Copy) {
  description = "For testing Flux with the getting-started example project."
  from layout.buildDirectory.dir("install")
  into "../examples/getting-started"
}
buildToolForGettingStarted.dependsOn installDist

test {
  finalizedBy jacocoTestReport
}

jacocoTestReport {
  dependsOn test
  reports {
    xml.required = true
  }
}

// See https://imperceptiblethoughts.com/shadow/configuration/dependencies/ .
shadowJar {
  configurations = [project.configurations.shadowDependencies]
  archiveBaseName = "marklogic-flux"
}

// Publishing setup - see https://docs.gradle.org/current/userguide/publishing_setup.html .
java {
  withJavadocJar()
  withSourcesJar()
}

publishing {
  publications {
    mainJava(MavenPublication) {
      groupId = group
      // Using a more fitting name of "flux-api". May eventually break out the current "flux-cli" module into
      // multiple Gradle subprojects, such as "flux-api" and "flux-cli".
      artifactId = "flux-api"
      version = version
      from components.java

      pom {
        name = "${group}:flux-api"
        description = "Flux API for data movement with MarkLogic"
        packaging = "jar"
        url = "https://github.com/marklogic/flux"
        licenses {
          license {
            name = "The Apache License, Version 2.0"
            url = "http://www.apache.org/licenses/LICENSE-2.0.txt"
          }
        }
        developers {
          developer {
            id = "marklogic"
            name = "MarkLogic Github Contributors"
            email = "general@developer.marklogic.com"
            organization = "MarkLogic"
            organizationUrl = "https://www.marklogic.com"
          }
        }
        scm {
          url = "git@github.com:marklogic/flux.git"
          connection = "scm:git@github.com:marklogic/flux.git"
          developerConnection = "scm:git@github.com:marklogic/flux.git"
        }

        // Mark some dependencies as optional.
        // May eventually do this for Azure and AWS dependencies as well.
        withXml {
          def dependenciesNode = asNode().dependencies[0]
          dependenciesNode.dependency.each { dep ->
            var artifactId = dep.artifactId.text()
            if (artifactId.startsWith('tika-parser-') || artifactId.equals('spark-avro_2.13')) {
              dep.appendNode('optional', 'true')
            }
          }
        }
      }
    }
  }
  repositories {
    maven {
      if (project.hasProperty("mavenUser")) {
        credentials {
          username = mavenUser
          password = mavenPassword
        }
        url publishUrl
        allowInsecureProtocol = true
      } else {
        name = "central"
        url = mavenCentralUrl
        credentials {
          username = mavenCentralUsername
          password = mavenCentralPassword
        }
      }
    }
  }
}

// Multiple shadow plugin users have requested the ability to not have the shadow jar published by default. This
// does the trick for that. See https://docs.gradle.org/current/userguide/publishing_customization.html .
// Also see https://github.com/johnrengelman/shadow/issues/586#issuecomment-70837559 for the shadow jar issue.
components.java.withVariantsFromConfiguration(configurations.shadowRuntimeElements) {
  skip()
}
